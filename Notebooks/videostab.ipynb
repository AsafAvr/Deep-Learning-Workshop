{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "import tifffile as tiff \n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from skimage import io\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from datetime import datetime\n",
    "# from deepcell.applications import NuclearSegmentation\n",
    "# from deepcell.applications import CellTracking\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert folder to p - all views inside will be analyzed and results will be saved to different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_path = Path('../../Ilan_Data/tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cord_for_crop(tif_files):\n",
    "    tff = tiff.imread(tif_files[0])\n",
    "    Y_size = tff.shape[0]\n",
    "    X_size = tff.shape[1]\n",
    "    cord_list = []\n",
    "    cord_list.append([0,int(X_size/2),0,int(Y_size/2)])\n",
    "    cord_list.append([0,int(X_size/2),int(Y_size/2), Y_size])\n",
    "    cord_list.append([int(X_size/2),X_size,0,int(Y_size/2)])\n",
    "    cord_list.append([int(X_size/2),X_size,int(Y_size/2), Y_size])\n",
    "    return cord_list\n",
    "\n",
    "def merge_tff_even_crop(tif_files ,cord_list, seq_length = 0):\n",
    "    if cord_list:\n",
    "        x1,x2,y1,y2 = cord_list\n",
    "    if tif_files:\n",
    "        tff = tiff.imread(tif_files[0])\n",
    "        tff = tff[y1:y2,x1:x2]\n",
    "        if len(tff.shape) == 2:\n",
    "            tff = np.expand_dims(tff, axis=-1)\n",
    "        tff = np.expand_dims(tff, axis=0)\n",
    "    if tif_files[1:]:\n",
    "        for i, other in enumerate(tif_files[1:]):\n",
    "            if i%2 == 0:\n",
    "                continue\n",
    "            othertff = tiff.imread(other)\n",
    "            othertff = othertff[y1:y2,x1:x2]\n",
    "            if len(othertff.shape) == 2:\n",
    "                othertff = np.expand_dims(othertff, axis=-1)\n",
    "            othertff = np.expand_dims(othertff, axis=0)\n",
    "            tff = np.concatenate((tff,othertff))\n",
    "\n",
    "    if seq_length > 0 and seq_length < len(tff):\n",
    "        tff = tff[0:seq_length,...]\n",
    "    return tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tff(tif_files , seq_length = 0):\n",
    "    if tif_files:\n",
    "        tff = tiff.imread(tif_files[0])\n",
    "        if len(tff.shape) == 2:\n",
    "            tff = np.expand_dims(tff, axis=-1)\n",
    "        tff = np.expand_dims(tff, axis=0)\n",
    "    if tif_files[1:]:\n",
    "        for i, other in enumerate(tif_files[1:]):\n",
    "            othertff = tiff.imread(other)\n",
    "            if len(othertff.shape) == 2:\n",
    "                othertff = np.expand_dims(othertff, axis=-1)\n",
    "            othertff = np.expand_dims(othertff, axis=0)\n",
    "            tff = np.concatenate((tff,othertff))\n",
    "\n",
    "    if seq_length > 0 and seq_length < len(tff):\n",
    "        tff = tff[0:seq_length,...]\n",
    "    return tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_channel(x,channel = 0):\n",
    "    return x[...,channel:(channel+1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(x, mpp =1.24):\n",
    "    app = NuclearSegmentation()\n",
    "    y_seg = app.predict(x, image_mpp = mpp)\n",
    "    return y_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking(x, y_seg):\n",
    "    tracker = CellTracking()\n",
    "    tracked_data = tracker.track(np.copy(x), y_seg)\n",
    "    return tracked_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tff(im1,im2,vmin,vmax):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(30, 15))\n",
    "    ax[0].imshow(im1)\n",
    "    ax[0].axis('off')\n",
    "    ax[0].set_title('Raw')\n",
    "    ax[1].imshow(im2, cmap='jet', vmin=vmin, vmax=vmax)\n",
    "    ax[1].set_title('Tracked')\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    fig.canvas.draw()  # draw the canvas, cache the renderer\n",
    "    image = np.frombuffer(fig.canvas.tostring_rgb(), dtype='uint8')\n",
    "    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    plt.close(fig)\n",
    "\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(x,y_seg, tracked_data, tff_path,well,view):\n",
    "    centroids = pd.DataFrame(columns = range(x.shape[0]))\n",
    "    morphologies = pd.DataFrame(columns = range(x.shape[0]))\n",
    "    embeddings = pd.DataFrame(columns = range(x.shape[0]))\n",
    "\n",
    "    for cell_id, cell_dict in tracked_data['tracks'].items():\n",
    "        for i,frame in enumerate(cell_dict['frames']):\n",
    "            centroids.at[cell_id,frame] = cell_dict['centroid'][i]\n",
    "            morphologies.at[cell_id,frame] = cell_dict['morphologies'][i]\n",
    "            embeddings.at[cell_id,frame] = cell_dict['embedding'][i]\n",
    "\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    directory = f\"Results_{well}_{view}_{date}\"\n",
    "    res_path = tff_path.joinpath(directory)\n",
    "    os.mkdir(res_path)\n",
    "\n",
    "    centroids.to_csv(res_path.joinpath('centroids.csv'))\n",
    "    morphologies.to_csv(res_path.joinpath('morphologies.csv'))\n",
    "    embeddings.to_csv(res_path.joinpath('embeddings.csv'))\n",
    "\n",
    "    with open(res_path.joinpath('track_results.pkl'), 'wb') as f:\n",
    "        pickle.dump(tracked_data, f)\n",
    "\n",
    "    X = tracked_data['X']  # raw X data\n",
    "    y = tracked_data['y_tracked']  # tracked y data\n",
    "\n",
    "    imageio.mimsave(res_path.joinpath('tracks.tif'), [plot_tff(x[i,...,0], y[i,...,0], y.min(), y.max())\n",
    "                                for i in range(y_seg.shape[0])])\n",
    "    imageio.mimsave(res_path.joinpath('tracks.gif'), [plot_tff(x[i,...,0], y[i,...,0], y.min(), y.max())\n",
    "                        for i in range(y_seg.shape[0])])\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tif_files = list(tif_path.glob('*.tif'))\n",
    "well_dict = {}\n",
    "for tif_file in tif_files:\n",
    "    file_name = tif_file.stem.split('_')\n",
    "    well_name = file_name[2]\n",
    "    view_name = file_name[3]\n",
    "    \n",
    "    if well_name not in well_dict:\n",
    "        well_dict[well_name] = {}\n",
    "    if view_name not in well_dict[well_name]:\n",
    "        well_dict[well_name][view_name] = []\n",
    "\n",
    "    well_dict[well_name][view_name].append(tif_file)\n",
    "\n",
    "for well, views in well_dict.items():\n",
    "    for view , view_tif in views.items():\n",
    "        print(f'well:{well} , view:{view}, {len(view_tif)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "well:G4 , view:2, 95\n",
      "<class 'numpy.ndarray'>\n",
      "(90, 1040, 1408, 1)\n"
     ]
    }
   ],
   "source": [
    "for well, views in well_dict.items():\n",
    "    for view , view_tif in views.items():\n",
    "        print(f'well:{well} , view:{view}, {len(view_tif)}')\n",
    "        view_tif.sort()\n",
    "        merged_tff = merge_tff(view_tif)\n",
    "        merged_tff = merged_tff[5:,...]\n",
    "        print(type(merged_tff))\n",
    "        print(merged_tff.shape)\n",
    "        break\n",
    "    break\n",
    " \n",
    "width = merged_tff.shape[2]\n",
    "hieght = merged_tff.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: FFMPEG: tag 0x58495644/'DVIX' is not found (format 'avi / AVI (Audio Video Interleaved)')'\n",
      "[ERROR:0@3114.897] global /Users/runner/work/opencv-python/opencv-python/opencv/modules/videoio/src/cap.cpp (597) open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): test.avi in function 'icvExtractPattern'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fps = 6\n",
    " \n",
    "# Syntax: VideoWriter_fourcc(c1, c2, c3, c4) # Concatenates 4 chars to a fourcc code\n",
    "#  cv2.VideoWriter_fourcc('M','J','P','G') or cv2.VideoWriter_fourcc(*'MJPG)\n",
    " \n",
    "fourcc = cv2.VideoWriter_fourcc(*'DVIX') # FourCC is a 4-byte code used to specify the video codec.\n",
    "# A video codec is software or hardware that compresses and decompresses digital video. \n",
    "# In the context of video compression, codec is a portmanteau of encoder and decoder, \n",
    "# while a device that only compresses is typically called an encoder, and one that only \n",
    "# decompresses is a decoder. Source - Wikipedia\n",
    " \n",
    "#Syntax: cv2.VideoWriter( filename, fourcc, fps, frameSize )\n",
    "video = cv2.VideoWriter('test.avi', fourcc, float(fps), (width, hieght))\n",
    "\n",
    "merged_tff = merged_tff.astype(np.uint8)\n",
    "for img in merged_tff:\n",
    "    video.write(img)\n",
    " \n",
    "video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for well, views in well_dict.items():\n",
    "    for view , view_tif in views.items():\n",
    "        print(f'well:{well} , view:{view}, {len(view_tif)}')\n",
    "        view_tif.sort()\n",
    "        cord_list = get_cord_for_crop(view_tif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for well, views in well_dict.items():\n",
    "    for view , view_tif in views.items():\n",
    "        print(f'well:{well} , view:{view}, {len(view_tif)}')\n",
    "        view_tif.sort()\n",
    "        cord_list = get_cord_for_crop(view_tif)\n",
    "        for i,cord in enumerate(cord_list):\n",
    "            print(cord)\n",
    "            merged_tff = merge_tff_even_crop(view_tif,cord) # , seq_length = 30\n",
    "            merged_tff = merged_tff[5:,...]\n",
    "            print(merged_tff.shape)\n",
    "            # tff = pick_channel(tff,0)\n",
    "            print(\"Segmentation\")\n",
    "            seg_tff = segmentation(merged_tff, mpp=1.24) \n",
    "            #(Drop if is empty)\n",
    "            print(\"Tracking\")\n",
    "            track_tff = tracking(merged_tff,seg_tff)\n",
    "            save_results(merged_tff,seg_tff,track_tff,tif_path,well,f'{view}_crop_{i}')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for well, views in well_dict.items():\n",
    "#     for view , view_tif in views.items():\n",
    "#         print(f'well:{well} , view:{view}, {len(view_tif)}')\n",
    "#         merged_tff = merge_tff(view_tif) #, seq_length= 30\n",
    "#         merged_tff = merged_tff[5:,...]\n",
    "#         # tff = pick_channel(tff,0)\n",
    "#         print(\"Segmentation\")\n",
    "#         seg_tff = segmentation(merged_tff, mpp=1.24) \n",
    "#         print(\"Tracking\")\n",
    "#         track_tff = tracking(merged_tff,seg_tff)\n",
    "#         save_results(merged_tff,seg_tff,track_tff,tif_path,well,view)\n",
    "#         del track_tff\n",
    "#         del seg_tff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 1040, 1408, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_tff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 1040, 1408, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seg_tff.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_tff = merged_tff[18:,...]\n",
    "seg_tff = seg_tff[18:,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1040, 1408, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(merged_tff.shape , seg_tff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1040, 1408, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_tff = tracking(merged_tff,seg_tff)\n",
    "save_results(merged_tff,seg_tff,track_tff,tif_path,well,view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.DataFrame(columns = range(merged_tff.shape[0]))\n",
    "\n",
    "for cell_id, cell_dict in track_tff['tracks'].items():\n",
    "    for i,frame in enumerate(cell_dict['frames']):\n",
    "        embeddings.at[cell_id,frame] = cell_dict['embedding'][i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
